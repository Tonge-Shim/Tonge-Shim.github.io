---
title: "Aligning *Everything* with Exact Preference Optimization:  
From Large-Language Models to Protein Design"
date: 2025-05-30
tags: [DPO, EXO, preference-learning, RLHF, protein-design, GitHub-Pages]
---

> **Research papers covered**  
> 1. **Direct Preference Optimization: Your Language Model is Secretly a Reward Model** – Rafailov *et al.* (NeurIPS 2023) :contentReference[oaicite:0]{index=0}  
> 2. **Data Distillation for Extrapolative Protein Design through Exact Preference Optimization** – Karimi *et al.* (ICLR 2025) :contentReference[oaicite:1]{index=1}  

---

## 1  Why read this post?  
Exact or **direct** preference-learning methods replace the two-stage RLHF pipeline (reward model ➜ PPO) with a **single closed-form loss**. The 2023 DPO paper showed this for language models; the 2025 protein-design paper pushes the idea to rugged biological fitness landscapes. This post walks through the *full* experimental evidence, highlights what generalizes across domains, and ends with a quick GitHub Pages deploy guide so you can publish the post on your own blog.

---

## 2  DPO for language models 🤖  

### 2.1  The objective in one line  
For a prompt \(x\) with preferred \(y^{+}\) and dispreferred \(y^{-}\):

\[
\mathcal L_{\text{DPO}}
= -\log \sigma\!\Bigl(\beta\bigl[\log\pi_\theta(y^{+}|x)-\log\pi_\theta(y^{-}|x)\bigr]
            -\log\pi_{\text{ref}}(y^{+}|x)+\log\pi_{\text{ref}}(y^{-}|x)\Bigr)
\]

One binary cross-entropy, no reward model, no on-policy sampling.

### 2.2  Experimental setup  

| Task | Dataset & feedback | Model | Metric |
|------|-------------------|-------|--------|
| Sentiment | IMDb, *oracle* classifier | GPT-2-L (774 M) | Expected reward vs KL |
| Summarization | Reddit TL;DR, human pairs | GPT-J (6 B) | GPT-4 win-rate |
| Dialogue | Anthropic HH | Pythia-2.8B | GPT-4 win-rate |

Hyper-parameters were nearly frozen: AdamW, lr = 2 × 10⁻⁵, β = 0.1, batch = 64. :contentReference[oaicite:2]{index=2}  

### 2.3  Headline numbers  

* **Sentiment:** at KL≈10 nats, DPO hits ≈0.9 normalized reward vs ≈0.75 for PPO.  
* **Summarization:** DPO summaries win **55 %** of GPT-4 votes vs 46 % (PPO) and 35 % (SFT).  
* **Dialogue:** the only method that beats the dataset’s “chosen” answers across 0.25 ≤ T ≤ 1.0.  
* **Generalization:** Reddit-trained DPO wins 36 % on CNN/DailyMail vs 26 % for PPO.  
* **Cost:** ~½ the FLOPs of PPO for the same 6 B policy.

---

## 3  Exact preference optimization for *proteins* 🧬  

### 3.1  Challenge: extrapolative protein design  
Generate sequences whose **fitness exceeds** everything in training.

| Dataset | Split | Train fitness range | Extrapolation region | Mutational gap |
|---------|-------|---------------------|----------------------|----------------|
| GFP | medium | 1.31 – 3.02 | > 3.02 | 6 |
| GFP | hard | 1.30 – 1.56 | > 1.56 | 7 |
| AAV capsid | medium | 5.64 – 7.48 | > 7.48 | 6 |
| AAV capsid | hard | 4.70 – 6.42 | > 6.42 | 7 | :contentReference[oaicite:3]{index=3}  

### 3.2  Progressive search pipeline  

1. **Local editor (pairs).** Fine-tune Prot-T5-XL on 0.9 M near-neighbor pairs to learn “small uphill” edits.  
2. **Hard-triplet mining.** From each seed \(x\), find \(x_l, x_w\) so that \(f(x_l)<f(x)<f(x_w)\) yet the editor ranks them incorrectly.  
3. **EXO / DPO training.** One epoch, β = 0.1, batch = 32 – the *only* training of this stage.  
4. **Iterative generation.** Ten edit rounds with top-\(k\)/top-\(p\) sampling; keep best candidates. :contentReference[oaicite:4]{index=4}  

### 3.3  Results (top-100 mean fitness)  

| Dataset | EXO (triplet-DPO) | Best previous | Δ gain |
|---------|------------------|---------------|--------|
| GFP-medium | **4.04 ± 0.01** | 2.39 (ICE) | **+69 %** |
| GFP-hard | 2.81 ± 0.07 | 2.55 (Align-PLM) | +10 % |
| AAV-medium | **13.18 ± 0.33** | 9.43 (ICE) | **+39 %** |
| AAV-hard | 10.95 ± 0.17 | 9.01 (Align-PLM) | +21 % | :contentReference[oaicite:5]{index=5}  

*EXO also multiplies extrapolation coverage (percent of outputs in the high-fitness region) by **5× – 17×** on medium splits.*

### 3.4  Ablation insights  

| Triplet dataset | AAV-hard Extrap % |
|-----------------|------------------|
| Mistakes (default) | 52.8 |
| All random triplets | 54.8 |
| Seed-boundary only | 6.7 |
| Scorer-distilled | 76.9 |
| Combined (mistakes + scorer) | **78.2** | :contentReference[oaicite:6]{index=6}  

→ Cheap “mistake” triplets already dominate previous methods; paying 200× GPU hours for scorer triplets helps only the very hardest split.

### 3.5  Take-home for bio-ML folks  
*Triplet* preference data encodes a **directional gradient** in rugged landscapes that pairwise data cannot. Offline EXO/DPO is stable, cheap, and pushes into unseen fitness territory without RL.

---

## 4  Cross-domain comparison  

| Aspect | LLM-DPO | Protein-EXO |
|--------|---------|-------------|
| Data signal | Human preference **pairs** | Scorer-mined **triplets** |
| Reference policy | SFT LM | Local editor (pairwise) |
| β value | 0.1 – 0.3 | 0.1 |
| Compute | 4 × A100 · 2 h | 1 × A100 · 1 epoch |
| Big win | Half-cost RLHF replacement | +40 % fitness jump |

---

## 5  Broader implications  

| Dimension | Insight |
|-----------|---------|
| **Generalization** | Preference learning scales from text to proteins; likely to images, molecules, and robotics next. |
| **Signal quality** | Hard-example mining (LLM hallucinations, low-fitness proteins) supplies rich supervision without humans. |
| **Practicality** | One-stage training is easier to reproduce and tunes a single hyper-parameter (β). |
| **Future** | Explore *higher-order* preferences (quadruplets) and integrate synthetic feedback loops (e.g., wet-lab or GPT-4). |

